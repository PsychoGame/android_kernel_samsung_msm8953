--- original/net/ipv4/tcp_input.c	2020-05-07 08:51:35.588250606 +0200
+++ changed/net/ipv4/tcp_input.c	2019-06-03 13:39:54.000000000 +0200
@@ -75,6 +75,9 @@
 #include <linux/ipsec.h>
 #include <asm/unaligned.h>
 #include <linux/errqueue.h>
+#ifdef CONFIG_NETPM
+#include <linux/inetdevice.h>
+#endif
 
 int sysctl_tcp_timestamps __read_mostly = 1;
 int sysctl_tcp_window_scaling __read_mostly = 1;
@@ -101,6 +104,10 @@ int sysctl_tcp_moderate_rcvbuf __read_mo
 int sysctl_tcp_early_retrans __read_mostly = 3;
 int sysctl_tcp_default_init_rwnd __read_mostly = TCP_INIT_CWND * 2;
 
+#ifdef CONFIG_NETPM
+int sysctl_tcp_netpm[4] __read_mostly;	/* Timestamp, RAT, PHY status, Access TP */
+#endif
+
 #define FLAG_DATA		0x01 /* Incoming frame contained data.		*/
 #define FLAG_WIN_UPDATE		0x02 /* Incoming ACK was a window update.	*/
 #define FLAG_DATA_ACKED		0x04 /* This ACK acknowledged new data.		*/
@@ -123,6 +130,141 @@ int sysctl_tcp_default_init_rwnd __read_
 #define TCP_REMNANT (TCP_FLAG_FIN|TCP_FLAG_URG|TCP_FLAG_SYN|TCP_FLAG_PSH)
 #define TCP_HP_BITS (~(TCP_RESERVED_BITS|TCP_FLAG_PSH))
 
+#ifdef CONFIG_NETPM
+static int netpm_int_log2(u32);
+static int netpm_pow(int, int);
+static int netpm_piecelinear_logbdp(struct tcp_sock *);
+
+#define NETPM_DEF_ENABLE 1
+#define NETPM_DEF_UB max_t(int, sysctl_tcp_rmem[2], 8388608)
+#define NETPM_DEF_LB 2097152
+#define NETPM_DEF_SRTT_SCALE 10 // it should be equal or larger than 1
+#define NETPM_DEF_PA 600000
+#define NETPM_DEF_PB 17
+#define NETPM_DEF_INC 0
+#define NETPM_DEF_INC_TH 1
+#define NETPM_DEF_DEC 0
+#define NETPM_DEF_DEC_TH 3
+#define NETPM_DEF_MP 150
+#define NETPM_DEF_GAIN 250
+#define NETPM_RTT_MIN_INITIAL_VAL 86400000
+
+static const s8 NetpmLogTable[256] = {
+	-1, 0, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3,
+	4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
+	5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
+	5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
+	6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
+	6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
+	6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
+	6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
+	7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
+	7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
+	7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
+	7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
+	7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
+	7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
+	7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
+	7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7
+};
+
+#ifdef SAMSUNG_NETPM_DEBUG
+#define netpm_debug(format, ...) pr_debug("<netpm> "format, __VA_ARGS__)
+#else
+#define netpm_debug(format, ...) do {} while (0)
+#endif
+
+static inline bool netpm(struct tcp_sock *tp)
+{
+	return NETPM_DEF_ENABLE && (sysctl_tcp_netpm[1] == 0x04) &&
+		(tp->netpm_netif == 1);
+}
+
+static inline int netpm_rmem_max(struct tcp_sock *tp)
+{
+	if (netpm(tp)) {
+		if (sysctl_tcp_netpm[2] == 0x01)
+			return tp->netpm_tcp_rmem_max;
+		else
+			return NETPM_DEF_LB;
+	}
+
+	return sysctl_tcp_rmem[2];
+}
+
+static struct net_device *netpm_dev_find(struct sock *sk)
+{
+	struct net_device *dev = NULL;
+
+	if (!sk)
+		goto outdev_out;
+
+	if (sk->sk_family == AF_INET) {
+		struct rtable *rt = (struct rtable *)__sk_dst_check(sk, 0);
+
+		if (rt)
+			dev = rt->dst.dev;
+
+		if (!dev) {
+			struct inet_sock *inet = inet_sk(sk);
+
+			dev = __ip_dev_find(sock_net(sk), inet->inet_saddr, false);
+		}
+	} else if (sk->sk_family == AF_INET6) {
+		struct ipv6_pinfo *np = inet6_sk(sk);
+		struct rtable *rt = (struct rtable *)__sk_dst_check(sk,
+				np->dst_cookie);
+
+		if (rt)
+			dev = rt->dst.dev;
+
+		if (!dev)
+			dev = ip6_dev_find(sock_net(sk), &np->saddr);
+	}
+outdev_out:
+	return dev;
+}
+
+static void netpm_init_buffer_space(struct sock *sk)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	struct net_device *dev_out = netpm_dev_find(sk);
+
+	if (!NETPM_DEF_ENABLE || !dev_out)
+		return;
+
+	if (dev_out->netpm_use) {
+		tp->netpm_netif = 1;
+
+		/* Initialization for NETPM */
+		tp->netpm_rtt_min = NETPM_RTT_MIN_INITIAL_VAL;
+		tp->netpm_max_tput = 0;
+		tp->netpm_srtt = 0;
+		tp->netpm_rttvar = 0;
+		tp->netpm_cwnd_est = 0;
+		tp->netpm_tcp_rmem_max_base = sysctl_tcp_rmem[2];
+		tp->netpm_tcp_rmem_max = sysctl_tcp_rmem[2];
+		tp->netpm_rwnd_max_adjust = 0;
+		tp->netpm_rbuf_flag = 0;
+
+		tp->netpm_rmem_max_curbdp[0] = -1;
+		tp->netpm_rmem_max_curbdp[1] = -1;
+	} else {
+		tp->netpm_netif = 0;
+	}
+}
+
+static inline u32 netpm_rtt_avg(struct tcp_sock *tp)
+{
+	return tp->netpm_srtt >> NETPM_DEF_SRTT_SCALE;
+}
+
+static inline u32 netpm_rttvar_avg(struct tcp_sock *tp)
+{
+	return tp->netpm_rttvar >> (NETPM_DEF_SRTT_SCALE - 1);
+}
+#endif
+
 /* Adapt the MSS value used to make delayed ack decision to the
  * real world.
  */
@@ -344,7 +486,11 @@ static int __tcp_grow_window(const struc
 	struct tcp_sock *tp = tcp_sk(sk);
 	/* Optimize this! */
 	int truesize = tcp_win_from_space(skb->truesize) >> 1;
+#ifdef CONFIG_NETPM
+	int window = tcp_win_from_space(netpm_rmem_max(tp)) >> 1;
+#else
 	int window = tcp_win_from_space(sysctl_tcp_rmem[2]) >> 1;
+#endif
 
 	while (tp->rcv_ssthresh <= window) {
 		if (truesize <= skb->len)
@@ -399,7 +545,11 @@ static void tcp_fixup_rcvbuf(struct sock
 		rcvmem <<= 2;
 
 	if (sk->sk_rcvbuf < rcvmem)
+#ifdef CONFIG_NETPM
+		sk->sk_rcvbuf = min(rcvmem, netpm_rmem_max(tcp_sk(sk)));
+#else
 		sk->sk_rcvbuf = min(rcvmem, sysctl_tcp_rmem[2]);
+#endif
 }
 
 /* 4. Try to fixup all. It is made immediately after connection enters
@@ -410,6 +560,10 @@ void tcp_init_buffer_space(struct sock *
 	struct tcp_sock *tp = tcp_sk(sk);
 	int maxwin;
 
+#ifdef CONFIG_NETPM
+	netpm_init_buffer_space(sk);
+#endif
+
 	if (!(sk->sk_userlocks & SOCK_RCVBUF_LOCK))
 		tcp_fixup_rcvbuf(sk);
 	if (!(sk->sk_userlocks & SOCK_SNDBUF_LOCK))
@@ -448,6 +602,15 @@ static void tcp_clamp_window(struct sock
 
 	icsk->icsk_ack.quick = 0;
 
+#ifdef CONFIG_NETPM
+	if (sk->sk_rcvbuf < netpm_rmem_max(tp) &&
+	    !(sk->sk_userlocks & SOCK_RCVBUF_LOCK) &&
+	    !sk_under_memory_pressure(sk) &&
+	    sk_memory_allocated(sk) < sk_prot_mem_limits(sk, 0)) {
+		sk->sk_rcvbuf = min(atomic_read(&sk->sk_rmem_alloc),
+				    netpm_rmem_max(tp));
+	}
+#else
 	if (sk->sk_rcvbuf < sysctl_tcp_rmem[2] &&
 	    !(sk->sk_userlocks & SOCK_RCVBUF_LOCK) &&
 	    !sk_under_memory_pressure(sk) &&
@@ -455,6 +618,7 @@ static void tcp_clamp_window(struct sock
 		sk->sk_rcvbuf = min(atomic_read(&sk->sk_rmem_alloc),
 				    sysctl_tcp_rmem[2]);
 	}
+#endif
 	if (atomic_read(&sk->sk_rmem_alloc) > sk->sk_rcvbuf)
 		tp->rcv_ssthresh = min(tp->window_clamp, 2U * tp->advmss);
 }
@@ -479,6 +643,41 @@ void tcp_initialize_rcv_mss(struct sock
 }
 EXPORT_SYMBOL(tcp_initialize_rcv_mss);
 
+#ifdef CONFIG_NETPM
+static void netpm_net_status_estimator(struct tcp_sock *tp)
+{
+	u32 netpm_rttdiff = 0;
+
+	if (tp->netpm_rtt_min > tp->rcv_rtt_est.rtt)
+		tp->netpm_rtt_min = tp->rcv_rtt_est.rtt;
+
+	if (tp->netpm_srtt != 0) {
+		tp->netpm_srtt -= netpm_rtt_avg(tp);
+		tp->netpm_srtt += tp->rcv_rtt_est.rtt;
+
+		if (tp->rcv_rtt_est.rtt >= netpm_rtt_avg(tp))
+			netpm_rttdiff = tp->rcv_rtt_est.rtt - netpm_rtt_avg(tp);
+		else
+			netpm_rttdiff = netpm_rtt_avg(tp) - tp->rcv_rtt_est.rtt;
+	} else {
+		tp->netpm_srtt = tp->rcv_rtt_est.rtt << NETPM_DEF_SRTT_SCALE;
+	}
+
+	if (tp->netpm_rttvar != 0) {
+		tp->netpm_rttvar -= netpm_rttvar_avg(tp);
+		tp->netpm_rttvar += netpm_rttdiff;
+	} else {
+		tp->netpm_rttvar = (tp->rcv_rtt_est.rtt << (NETPM_DEF_SRTT_SCALE - 1)) / 2;
+	}
+
+	netpm_debug("%s tp->rcv_rtt_est.rtt = %u\n", __func__, tp->rcv_rtt_est.rtt);
+	netpm_debug("%s tp->rtt_min = %u\n", __func__, tp->netpm_rtt_min);
+	netpm_debug("%s rtt_min_ms = %u\n", __func__, rtt_min_ms);
+	netpm_debug("%s tp->netpm_srtt = %u\n", __func__, netpm_rtt_avg(tp));
+	netpm_debug("%s tp->netpm_rttvar = %u\n", __func__, netpm_rttvar_avg(tp));
+}
+#endif
+
 /* Receiver "autotuning" code.
  *
  * The algorithm for RTT estimation w/o timestamps is based on
@@ -524,6 +723,10 @@ static void tcp_rcv_rtt_update(struct tc
 
 	if (tp->rcv_rtt_est.rtt != new_sample)
 		tp->rcv_rtt_est.rtt = new_sample;
+#ifdef CONFIG_NETPM
+	if (netpm(tp))
+		netpm_net_status_estimator(tp);
+#endif
 }
 
 static inline void tcp_rcv_rtt_measure(struct tcp_sock *tp)
@@ -549,6 +752,155 @@ static inline void tcp_rcv_rtt_measure_t
 		tcp_rcv_rtt_update(tp, tcp_time_stamp - tp->rx_opt.rcv_tsecr, 0);
 }
 
+#ifdef CONFIG_NETPM
+static int netpm_int_log2(u32 v)
+{
+	u32 r, t, tt;
+
+	tt = v >> 16;
+	if (tt)
+		r = ((t = tt >> 8) ? 24 + NetpmLogTable[t] : 16 + NetpmLogTable[tt]);
+	else
+		r = ((t = v >> 8) ? 8 + NetpmLogTable[t] : NetpmLogTable[v]);
+
+	return r;
+}
+
+static int netpm_pow(int base, int n)
+{
+	int result = 1, i;
+
+	for (i = 0; i < n; i++)
+		result *= base;
+
+	return result;
+}
+
+/* RWNDmax = a * log(TPaccess,max * RTTmin - b) */
+#define NETPM_RWND_CAL(rtt)	\
+		(NETPM_DEF_PA * \
+		  (netpm_int_log2(tp->netpm_max_tput * rtt * 125) \
+		  - NETPM_DEF_PB))
+
+static int netpm_piecelinear_logbdp(struct tcp_sock *tp)
+{
+	int rtt_min_ms, intlog_lower, intlog_upper, s, i, delta;
+	u32 rtt_low, rtt_high;
+
+	rtt_min_ms = jiffies_to_msecs(tp->netpm_rtt_min >> 3);
+	s = 0;
+	i = 0;
+	while (s < rtt_min_ms) {
+		i++;
+		s = 7 * netpm_pow(2, i);
+	}
+
+	rtt_high = s;
+	rtt_low = 7 * netpm_pow(2, i - 1);
+
+	tp->netpm_max_tput = sysctl_tcp_netpm[3];
+
+	intlog_lower = NETPM_RWND_CAL(rtt_low);
+	intlog_upper = NETPM_RWND_CAL(rtt_high);
+
+	if (intlog_lower < 0)
+		return 0;
+
+	delta = (rtt_min_ms - rtt_low) * (intlog_upper - intlog_lower)
+		/ (rtt_high - rtt_low);
+
+	return intlog_lower + delta;
+}
+
+static inline void netpm_increase_rwnd_max(struct tcp_sock *tp)
+{
+	tp->netpm_tcp_rmem_max = tp->netpm_tcp_rmem_max * 11 / 10;
+}
+
+static inline void netpm_decrease_rwnd_max(struct tcp_sock *tp)
+{
+	tp->netpm_tcp_rmem_max = tp->netpm_tcp_rmem_max * 8 / 10;
+}
+
+static void netpm_rwnd_max_adjustment(struct tcp_sock *tp)
+{
+	int rtt_min_ms, srtt_ms, rtt_var_ms;
+
+	rtt_min_ms = jiffies_to_msecs(tp->netpm_rtt_min >> 3);
+	srtt_ms = jiffies_to_msecs(netpm_rtt_avg(tp) >> 3);
+	rtt_var_ms = jiffies_to_msecs(netpm_rttvar_avg(tp) >> 3);
+
+	if (tp->netpm_srtt && tp->netpm_rtt_min != NETPM_RTT_MIN_INITIAL_VAL) {
+		/* initial RWND max estimation */
+		tp->netpm_rmem_max_curbdp[1] = tp->netpm_rmem_max_curbdp[0];
+		if (rtt_min_ms <= NETPM_DEF_MP)
+			tp->netpm_rmem_max_curbdp[0] = sysctl_tcp_netpm[3] * rtt_min_ms * NETPM_DEF_GAIN;
+		else
+			tp->netpm_rmem_max_curbdp[0] = netpm_piecelinear_logbdp(tp);
+
+		if (tp->netpm_rmem_max_curbdp[0] != tp->netpm_rmem_max_curbdp[1])
+			tp->netpm_rwnd_max_adjust = 0;
+
+		netpm_debug("%s saddr/sport = %08X/%d\n", __func__,
+			    ntohl(tp->inet_conn.icsk_inet.inet_saddr),
+			    ntohs(tp->inet_conn.icsk_inet.inet_sport));
+		netpm_debug("%s daddr/dport = %08X/%d\n", __func__,
+			    ntohl(tp->inet_conn.icsk_inet.inet_daddr),
+			    ntohs(tp->inet_conn.icsk_inet.inet_dport));
+		netpm_debug("%s netpm_max_tput = %d, rtt_min_ms = %d, srtt_ms = %d, rtt_var_ms = %d\n",
+			    __func__, tp->netpm_max_tput, rtt_min_ms, srtt_ms, rtt_var_ms);
+
+		tp->netpm_tcp_rmem_max_base = tcp_space_from_win(tp->netpm_rmem_max_curbdp[0]);
+
+		netpm_debug("%s calculated netpm_tcp_rmem_max_base = %d\n",
+			    __func__, tp->netpm_tcp_rmem_max_base);
+
+		if (tp->netpm_tcp_rmem_max_base > NETPM_DEF_UB)
+			tp->netpm_tcp_rmem_max_base = NETPM_DEF_UB;
+		else if (tp->netpm_tcp_rmem_max_base < NETPM_DEF_LB)
+			tp->netpm_tcp_rmem_max_base = NETPM_DEF_LB;
+
+		if (tp->netpm_rwnd_max_adjust == 0)
+			tp->netpm_tcp_rmem_max = tp->netpm_tcp_rmem_max_base;
+	}
+
+	/* Dynamic RWND increase */
+	if (NETPM_DEF_INC && tp->netpm_rtt_min != NETPM_RTT_MIN_INITIAL_VAL &&
+	    tp->netpm_cwnd_est > tcp_win_from_space(tp->netpm_tcp_rmem_max) - 100 * tp->advmss &&
+	    srtt_ms - rtt_min_ms < NETPM_DEF_INC_TH * rtt_var_ms) {
+		/* increasing size calculation */
+		netpm_increase_rwnd_max(tp);
+
+		if (tp->netpm_tcp_rmem_max > NETPM_DEF_UB)
+			tp->netpm_tcp_rmem_max = NETPM_DEF_UB;
+
+		tp->netpm_rwnd_max_adjust = 1;
+
+		netpm_debug("%s netpm_tcp_rmem_max increased = %d, netpm_cwnd_est = %d\n",
+			    __func__, tp->netpm_tcp_rmem_max, tp->netpm_cwnd_est);
+	} else if (NETPM_DEF_DEC &&
+		   tp->netpm_rwnd_max_adjust == 0 &&
+		   tp->netpm_rtt_min != NETPM_RTT_MIN_INITIAL_VAL &&
+		   srtt_ms - rtt_min_ms > NETPM_DEF_DEC_TH * rtt_var_ms) {
+		/* decreasing size calculation */
+		netpm_decrease_rwnd_max(tp);
+
+		if (tp->netpm_tcp_rmem_max < NETPM_DEF_LB)
+			tp->netpm_tcp_rmem_max = NETPM_DEF_LB;
+
+		tp->netpm_rwnd_max_adjust = 1;
+
+		netpm_debug("%s netpm_tcp_rmem_max decreased = %d\n",
+			    __func__, tp->netpm_tcp_rmem_max);
+	}
+
+	netpm_debug("%s filtered netpm_tcp_rmem_max_base = %d\n", __func__,
+		    tp->netpm_tcp_rmem_max_base);
+	netpm_debug("%s netpm_tcp_rmem_max = %d\n", __func__,
+		    tp->netpm_tcp_rmem_max);
+}
+#endif
+
 /*
  * This function should be called every time data is copied to user space.
  * It calculates the appropriate TCP receive buffer space.
@@ -565,8 +917,25 @@ void tcp_rcv_space_adjust(struct sock *s
 
 	/* Number of bytes copied to user in last RTT */
 	copied = tp->copied_seq - tp->rcvq_space.seq;
-	if (copied <= tp->rcvq_space.space)
-		goto new_measure;
+#ifdef CONFIG_NETPM
+	if (netpm(tp)) {
+		if (tp->netpm_cwnd_est == 0)
+			tp->netpm_cwnd_est = copied;
+		else
+			tp->netpm_cwnd_est = (7 * tp->netpm_cwnd_est + copied) / 8;
+
+		netpm_debug("%s cwnd_est = %d\n", __func__, tp->netpm_cwnd_est);
+
+		if (copied <= tp->rcvq_space.space &&
+		    tp->netpm_max_tput == sysctl_tcp_netpm[3])
+			goto new_measure;
+	} else {
+#endif
+		if (copied <= tp->rcvq_space.space)
+			goto new_measure;
+#ifdef CONFIG_NETPM
+	}
+#endif
 
 	/* A bit of theory :
 	 * copied = bytes received in previous RTT, our base window
@@ -613,6 +982,13 @@ void tcp_rcv_space_adjust(struct sock *s
 			/* Make the window clamp follow along.  */
 			tp->window_clamp = tcp_win_from_space(rcvbuf);
 		}
+#ifdef CONFIG_NETPM
+		else if (netpm(tp) && netpm_rmem_max(tp) < sk->sk_rcvbuf) {
+			sk->sk_rcvbuf = netpm_rmem_max(tp);
+
+			tp->window_clamp = sk->sk_rcvbuf / rcvmem * tp->advmss;
+		}
+#endif
 	}
 	tp->rcvq_space.space = copied;
 
@@ -3346,8 +3722,6 @@ static void tcp_send_challenge_ack(struc
 	u32 now = jiffies / HZ;
 	u32 count;
 
-	/* Check host-wide RFC 5961 rate limit. */
-	now = jiffies / HZ;
 	if (now != challenge_timestamp) {
 		u32 half = (sysctl_tcp_challenge_ack_limit + 1) >> 1;
 
@@ -4998,7 +5372,7 @@ static int tcp_copy_to_iovec(struct sock
 		err = skb_copy_datagram_iovec(skb, hlen, tp->ucopy.iov, chunk);
 	else
 		err = skb_copy_and_csum_datagram_iovec(skb, hlen,
-						       tp->ucopy.iov);
+						       tp->ucopy.iov, chunk);
 
 	if (!err) {
 		tp->ucopy.len -= chunk;
