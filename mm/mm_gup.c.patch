--- original/mm/gup.c	2020-05-07 08:31:27.591771001 +0200
+++ changed/mm/gup.c	2019-06-03 13:39:53.000000000 +0200
@@ -14,8 +14,90 @@
 #include <linux/rwsem.h>
 #include <asm/pgtable.h>
 
+#ifdef CONFIG_CMA_PINPAGE_MIGRATION
+#include <linux/migrate.h>
+#include <linux/mm_inline.h>
+#include <linux/mmu_notifier.h>
+#include <asm/tlbflush.h>
+#include <linux/delay.h>
+#endif
+
 #include "internal.h"
 
+#ifdef CONFIG_CMA_PINPAGE_MIGRATION
+static struct page *__alloc_nonmovable_userpage(struct page *page,
+				unsigned long private, int **result)
+{
+	return alloc_page(GFP_HIGHUSER);
+}
+
+static bool __need_migrate_cma_page(struct page *page,
+				struct vm_area_struct *vma,
+				unsigned long start, unsigned int flags)
+{
+	if (!(flags & FOLL_CMA))
+		return false;
+
+	if (!(flags & FOLL_GET))
+		return false;
+
+	if (!is_cma_pageblock(page))
+		return false;
+
+	if ((vma->vm_flags & VM_STACK_INCOMPLETE_SETUP) ==
+					VM_STACK_INCOMPLETE_SETUP)
+		return false;
+
+	if (!PageLRU(page)) {
+		migrate_prep_local();
+		if (WARN_ON(!PageLRU(page))) {
+			return false;
+		}
+	}
+
+	return true;
+}
+
+static int __migrate_cma_pinpage(struct page *page, struct vm_area_struct *vma)
+{
+	struct zone *zone = page_zone(page);
+	struct list_head migratepages;
+	struct lruvec *lruvec;
+	int tries = 0;
+	int ret = 0;
+
+	spin_lock_irq(&zone->lru_lock);
+	ret = __isolate_lru_page(page, 0);
+	if (ret) {
+		spin_unlock_irq(&zone->lru_lock);
+		return ret;
+	}
+
+	INIT_LIST_HEAD(&migratepages);
+
+	lruvec = mem_cgroup_page_lruvec(page, page_zone(page));
+	del_page_from_lru_list(page, lruvec, page_lru(page));
+	spin_unlock_irq(&zone->lru_lock);
+
+	list_add(&page->lru, &migratepages);
+	inc_zone_page_state(page, NR_ISOLATED_ANON + page_is_file_cache(page));
+
+	while (!list_empty(&migratepages) && tries++ < 5) {
+		ret = migrate_pages(&migratepages, __alloc_nonmovable_userpage,
+					NULL, 0, MIGRATE_SYNC, MR_CMA);
+	}
+
+	if (ret < 0) {
+		putback_movable_pages(&migratepages);
+		pr_err("%s: migration failed %p[%#lx]\n", __func__,
+					page, page_to_pfn(page));
+		return -EFAULT;
+	}
+
+	return 0;
+}
+#endif
+
 static struct page *no_page_table(struct vm_area_struct *vma,
 		unsigned int flags)
 {
@@ -49,6 +131,11 @@ static struct page *follow_page_pte(stru
 	struct page *page;
 	spinlock_t *ptl;
 	pte_t *ptep, pte;
+#ifdef CONFIG_CMA_PINPAGE_MIGRATION
+	struct page *failed_page = NULL;
+	struct page *old_page = NULL;
+	int retry_cnt = 0;
+#endif
 
 retry:
 	if (unlikely(pmd_bad(*pmd)))
@@ -89,6 +176,49 @@ retry:
 		page = pte_page(pte);
 	}
 
+#ifdef CONFIG_CMA_PINPAGE_MIGRATION
+	/* if still the isolation failed page, then retry */
+	if (failed_page && (page == failed_page)) {
+		retry_cnt++;
+		msleep_interruptible(20);
+		goto retry;
+	}
+	if (__need_migrate_cma_page(page, vma, address, flags) == false)
+		goto skip_pinpage;
+
+	pte_unmap_unlock(ptep, ptl);
+	switch (__migrate_cma_pinpage(page, vma)) {
+		case -EINVAL:
+		case -EBUSY:
+			pr_warn("%s: failed to isolate lru page\n", __func__);
+			dump_page(page, "failed to isolate lru page");
+			failed_page = page;
+			retry_cnt++;
+			goto retry;
+		case -EFAULT:
+			ptep = pte_offset_map_lock(mm, pmd, address, &ptl);
+			break;
+		default:
+			old_page = page;
+			migration_entry_wait(mm, pmd, address);
+			ptep = pte_offset_map_lock(mm, pmd, address, &ptl);
+			update_mmu_cache(vma, address, ptep);
+			pte = *ptep;
+			set_pte_at_notify(mm, address, ptep, pte);
+			page = vm_normal_page(vma, address, pte);
+			BUG_ON(!page);
+
+			pr_debug("cma: cma page %p[%#lx] migrated to new "
+					"page %p[%#lx]\n", old_page,
+					page_to_pfn(old_page),
+					page, page_to_pfn(page));
+	}
+	if (failed_page)
+		pr_warn("cma: isolation failed page %p[%#lx] , fixed to page %p[%#lx] (retry %d)\n",
+			failed_page, page_to_pfn(failed_page),
+			page, page_to_pfn(page), retry_cnt);
+skip_pinpage:
+#endif
 	if (flags & FOLL_GET)
 		get_page_foll(page);
 	if (flags & FOLL_TOUCH) {
@@ -132,6 +262,11 @@ bad_page:
 
 no_page:
 	pte_unmap_unlock(ptep, ptl);
+#ifdef CONFIG_CMA_PINPAGE_MIGRATION
+	if (failed_page)
+		pr_warn("cma: isolation failed page %p[%#lx] , it was reclaimed (retry %d)\n",
+			failed_page, page_to_pfn(failed_page), retry_cnt);
+#endif
 	if (!pte_none(pte))
 		return NULL;
 	return no_page_table(vma, flags);
@@ -440,6 +575,9 @@ long __get_user_pages(struct task_struct
 	if (!(gup_flags & FOLL_FORCE))
 		gup_flags |= FOLL_NUMA;
 
+	if ((gup_flags & FOLL_CMA) != 0)
+		migrate_prep();
+
 	do {
 		struct page *page;
 		unsigned int foll_flags = gup_flags;
