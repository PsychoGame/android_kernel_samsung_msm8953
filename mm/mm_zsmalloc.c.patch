--- original/mm/zsmalloc.c	2020-05-07 08:06:22.642179739 +0200
+++ changed/mm/zsmalloc.c	2019-06-03 13:39:54.000000000 +0200
@@ -75,7 +75,11 @@
  * span more than 1 page which avoids complex case of mapping 2 pages simply
  * to restore link_free pointer values.
  */
+#ifdef CONFIG_ZSMALLOC_OBJ_SEQ
+#define ZS_ALIGN		16
+#else
 #define ZS_ALIGN		8
+#endif
 
 /*
  * A single 'zspage' is composed of up to 2^N discontiguous 0-order (single)
@@ -159,11 +163,22 @@
 enum fullness_group {
 	ZS_ALMOST_FULL,
 	ZS_ALMOST_EMPTY,
+	ZS_FULL,
 	_ZS_NR_FULLNESS_GROUPS,
 
 	ZS_EMPTY,
-	ZS_FULL
+	ZS_RECLAIM,
 };
+#define _ZS_NR_AVAILABLE_FULLNESS_GROUPS ZS_FULL
+
+#ifdef CONFIG_ZSMALLOC_OBJ_SEQ
+#define ZS_OBJ_SEQ_SIZE (sizeof(unsigned int))
+static u64 seq_sum;
+static u64 objs_used;
+static u64 obj_scanned;
+static u64 obj_success;
+#endif
+
 
 enum zs_stat_type {
 	OBJ_ALLOCATED,
@@ -255,9 +270,14 @@ struct zs_pool {
 	gfp_t flags;	/* allocation flags used when growing pool */
 	atomic_long_t pages_allocated;
 
+	struct zs_ops *ops;
+
 #ifdef CONFIG_ZSMALLOC_STAT
 	struct dentry *stat_dentry;
 #endif
+#ifdef CONFIG_ZSMALLOC_OBJ_SEQ
+	unsigned int recent_seq; /* start from 1.*/
+#endif
 };
 
 /*
@@ -280,6 +300,16 @@ struct mapping_area {
 	bool huge;
 };
 
+/* atomic counter indicating which class/fg to reclaim from */
+static atomic_t lru_class_fg;
+/* specific order of fg we want to reclaim from */
+static enum fullness_group lru_fg[] = {
+	ZS_ALMOST_EMPTY,
+	ZS_ALMOST_FULL,
+	ZS_FULL,
+};
+#define _ZS_NR_LRU_CLASS_FG (zs_size_classes * ARRAY_SIZE(lru_fg))
+
 static int create_handle_cache(struct zs_pool *pool)
 {
 	pool->handle_cachep = kmem_cache_create("zs_handle", ZS_HANDLE_SIZE,
@@ -289,8 +319,7 @@ static int create_handle_cache(struct zs
 
 static void destroy_handle_cache(struct zs_pool *pool)
 {
-	if (pool->handle_cachep)
-		kmem_cache_destroy(pool->handle_cachep);
+	kmem_cache_destroy(pool->handle_cachep);
 }
 
 static unsigned long alloc_handle(struct zs_pool *pool)
@@ -306,16 +335,30 @@ static void free_handle(struct zs_pool *
 
 static void record_obj(unsigned long handle, unsigned long obj)
 {
-	*(unsigned long *)handle = obj;
+	/*
+	 * lsb of @obj represents handle lock while other bits
+	 * represent object value the handle is pointing so
+	 * updating shouldn't do store tearing.
+	 */
+	WRITE_ONCE(*(unsigned long *)handle, obj);
 }
 
 /* zpool driver */
 
 #ifdef CONFIG_ZPOOL
 
+static int zs_zpool_evict(struct zs_pool *pool, unsigned long handle)
+{
+	return zpool_evict(pool, handle);
+}
+
+static struct zs_ops zs_zpool_ops = {
+	.evict =	zs_zpool_evict
+};
+
 static void *zs_zpool_create(char *name, gfp_t gfp, struct zpool_ops *zpool_ops)
 {
-	return zs_create_pool(name, gfp);
+	return zs_create_pool(name, gfp, &zs_zpool_ops);
 }
 
 static void zs_zpool_destroy(void *pool)
@@ -334,11 +377,43 @@ static void zs_zpool_free(void *pool, un
 	zs_free(pool, handle);
 }
 
+#ifdef CONFIG_ZSMALLOC_OBJ_SEQ
+static int zs_zpool_shrink(void *pool, unsigned int pages,
+			unsigned int *reclaimed)
+{
+	int total = 0, ret = 0;
+
+	while (pages--) {
+		ret = zs_shrink(pool);
+		if (ret < 0)
+			ret = 0;
+		total += ret;
+	}
+
+	if (reclaimed)
+		*reclaimed = total;
+	return total ? 0 : -ENOENT;
+}
+#else
 static int zs_zpool_shrink(void *pool, unsigned int pages,
 			unsigned int *reclaimed)
 {
-	return -EINVAL;
+	int total = 0, ret = 0;
+
+	while (total < pages) {
+		ret = zs_shrink(pool);
+		WARN_ON(!ret);
+		if (ret <= 0)
+			break;
+		total += ret;
+		ret = 0;
+	}
+
+	if (reclaimed)
+		*reclaimed = total;
+	return ret;
 }
+#endif
 
 static void *zs_zpool_map(void *pool, unsigned long handle,
 			enum zpool_mapmode mm)
@@ -349,6 +424,11 @@ static void *zs_zpool_map(void *pool, un
 	case ZPOOL_MM_RO:
 		zs_mm = ZS_MM_RO;
 		break;
+#ifdef CONFIG_ZSWAP_SAME_PAGE_SHARING
+	case ZPOOL_MM_RO_NOWAIT:
+		zs_mm = ZS_MM_RO_NOWAIT;
+		break;
+#endif
 	case ZPOOL_MM_WO:
 		zs_mm = ZS_MM_WO;
 		break;
@@ -370,6 +450,16 @@ static u64 zs_zpool_total_size(void *poo
 	return zs_get_total_pages(pool) << PAGE_SHIFT;
 }
 
+static unsigned long zs_zpool_compact(void *pool)
+{
+	return zs_compact(pool);
+}
+
+static bool zs_zpool_compactable(void *pool, unsigned int pages)
+{
+	return zs_compactable(pool, pages);
+}
+
 static struct zpool_driver zs_zpool_driver = {
 	.type =		"zsmalloc",
 	.owner =	THIS_MODULE,
@@ -381,6 +471,8 @@ static struct zpool_driver zs_zpool_driv
 	.map =		zs_zpool_map,
 	.unmap =	zs_zpool_unmap,
 	.total_size =	zs_zpool_total_size,
+	.compact =	zs_zpool_compact,
+	.compactable =	zs_zpool_compactable,
 };
 
 MODULE_ALIAS("zpool-zsmalloc");
@@ -491,6 +583,7 @@ static int zs_stats_size_show(struct seq
 	unsigned long obj_allocated, obj_used, pages_used;
 	unsigned long total_class_almost_full = 0, total_class_almost_empty = 0;
 	unsigned long total_objs = 0, total_used_objs = 0, total_pages = 0;
+	unsigned long total_unused = 0;
 
 	seq_printf(s, " %5s %5s %11s %12s %13s %10s %10s %16s\n",
 			"class", "size", "almost_full", "almost_empty",
@@ -525,13 +618,24 @@ static int zs_stats_size_show(struct seq
 		total_objs += obj_allocated;
 		total_used_objs += obj_used;
 		total_pages += pages_used;
+
+		total_unused += ((obj_allocated - obj_used) * class->size);
 	}
 
 	seq_puts(s, "\n");
-	seq_printf(s, " %5s %5s %11lu %12lu %13lu %10lu %10lu\n",
+	seq_printf(s, " %5s %5s %11lu %12lu %13lu %10lu %10lu %10lu\n",
 			"Total", "", total_class_almost_full,
 			total_class_almost_empty, total_objs,
-			total_used_objs, total_pages);
+			total_used_objs, total_pages, total_unused);
+#ifdef CONFIG_ZSMALLOC_OBJ_SEQ
+	seq_printf(s, "OBJ_SEQ: objs_used %lu seq_sum %lu avg %lu recent %lu " \
+			"obj_scanned %lu obj_success %lu\n",
+			(unsigned long)objs_used, (unsigned long)seq_sum,
+			(unsigned long)(objs_used ? (seq_sum / objs_used) : 0),
+			(unsigned long)pool->recent_seq,
+			(unsigned long)obj_scanned,
+			(unsigned long)obj_success);
+#endif
 
 	return 0;
 }
@@ -616,7 +720,6 @@ static inline void zs_pool_stat_destroy(
 
 #endif
 
-
 /*
  * For each size class, zspages are divided into different groups
  * depending on how "full" they are. This was done so that we could
@@ -666,7 +769,9 @@ static void insert_zspage(struct page *p
 		list_add_tail(&page->lru, &(*head)->lru);
 
 	*head = page;
-	zs_stat_inc(class, fullness == ZS_ALMOST_EMPTY ?
+
+	if (fullness != ZS_FULL)
+		zs_stat_inc(class, fullness == ZS_ALMOST_EMPTY ?
 			CLASS_ALMOST_EMPTY : CLASS_ALMOST_FULL, 1);
 }
 
@@ -693,7 +798,9 @@ static void remove_zspage(struct page *p
 					struct page, lru);
 
 	list_del_init(&page->lru);
-	zs_stat_dec(class, fullness == ZS_ALMOST_EMPTY ?
+
+	if (fullness != ZS_FULL)
+		zs_stat_dec(class, fullness == ZS_ALMOST_EMPTY ?
 			CLASS_ALMOST_EMPTY : CLASS_ALMOST_FULL, 1);
 }
 
@@ -716,14 +823,12 @@ static enum fullness_group fix_fullness_
 
 	get_zspage_mapping(page, &class_idx, &currfg);
 	newfg = get_fullness_group(page);
-	if (newfg == currfg)
-		goto out;
-
+	/* Need to do this even if currfg == newfg, to update lru */
 	remove_zspage(page, class, currfg);
 	insert_zspage(page, class, newfg);
-	set_zspage_mapping(page, class_idx, newfg);
+	if (currfg != newfg)
+		set_zspage_mapping(page, class_idx, newfg);
 
-out:
 	return newfg;
 }
 
@@ -1008,12 +1113,12 @@ cleanup:
 	return first_page;
 }
 
-static struct page *find_get_zspage(struct size_class *class)
+static struct page *find_available_zspage(struct size_class *class)
 {
 	int i;
 	struct page *page;
 
-	for (i = 0; i < _ZS_NR_FULLNESS_GROUPS; i++) {
+	for (i = 0; i < _ZS_NR_AVAILABLE_FULLNESS_GROUPS; i++) {
 		page = class->fullness_list[i];
 		if (page)
 			break;
@@ -1022,6 +1127,66 @@ static struct page *find_get_zspage(stru
 	return page;
 }
 
+/*
+ * This simply iterates atomically through all classes,
+ * using a specific fullness group. At the end, it starts
+ * over using the next fullness group, and so on. The
+ * fullness groups are used in a specific order, from
+ * least to most full.
+ */
+static void find_next_lru_class_fg(struct zs_pool *pool,
+			struct size_class **class, enum fullness_group *fg)
+{
+	int i = atomic_inc_return(&lru_class_fg);
+
+	if (i >= _ZS_NR_LRU_CLASS_FG) {
+		int orig = i;
+
+		i %= _ZS_NR_LRU_CLASS_FG;
+		/*
+		 * only need to try once, since if we don't
+		 * succeed whoever changed it will also try
+		 * and eventually someone will reset it.
+		 */
+		atomic_cmpxchg(&lru_class_fg, orig, i);
+	}
+	*class = pool->size_class[i % zs_size_classes];
+	*fg = lru_fg[i / zs_size_classes];
+}
+
+/*
+ * This attempts to find the LRU zspage, but that's not really possible
+ * because zspages are not contained in a single LRU list, they're
+ * contained inside fullness groups which are themselves contained
+ * inside classes. So this simply iterates through the classes and
+ * fullness groups to find the next non-empty fullness group, and
+ * uses the LRU zspage there.
+ *
+ * On success, the zspage is returned with its class locked.
+ * On failure, NULL is returned.
+ */
+static struct page *find_lru_zspage(struct zs_pool *pool)
+{
+	struct size_class *class;
+	struct page *page;
+	enum fullness_group fg;
+	int tries = 0;
+
+	while (tries++ < _ZS_NR_LRU_CLASS_FG) {
+		find_next_lru_class_fg(pool, &class, &fg);
+
+		spin_lock(&class->lock);
+
+		page = class->fullness_list[fg];
+		if (page)
+			return list_prev_entry(page, lru);
+
+		spin_unlock(&class->lock);
+	}
+
+	return NULL;
+}
+
 #ifdef CONFIG_PGTABLE_MAPPING
 static inline int __zs_cpu_up(struct mapping_area *area)
 {
@@ -1127,6 +1292,11 @@ static void __zs_unmap_object(struct map
 		size -= ZS_HANDLE_SIZE;
 		off += ZS_HANDLE_SIZE;
 	}
+#ifdef CONFIG_ZSMALLOC_OBJ_SEQ
+	buf += ZS_OBJ_SEQ_SIZE;
+	size -= ZS_OBJ_SEQ_SIZE;
+	off += ZS_OBJ_SEQ_SIZE;
+#endif
 
 	sizes[0] = PAGE_SIZE - off;
 	sizes[1] = size - sizes[0];
@@ -1239,6 +1409,71 @@ unsigned long zs_get_total_pages(struct
 }
 EXPORT_SYMBOL_GPL(zs_get_total_pages);
 
+enum obj_seq_op {
+	OBJ_SEQ_GET,
+	OBJ_SEQ_SET,
+	OBJ_SEQ_CLEAR,
+};
+#ifdef CONFIG_ZSMALLOC_OBJ_SEQ
+
+/* class->lock should be held before calling this */
+static inline unsigned int obj_seq_operation(struct zs_pool *pool,
+		struct size_class *class, unsigned long obj, enum obj_seq_op op)
+{
+	struct page *m_page;
+	unsigned long m_objidx, m_offset;
+	void *vaddr;
+	unsigned int *target;
+	unsigned int seq, ret = 0;
+
+	obj_to_location(obj, &m_page, &m_objidx);
+	m_offset = obj_idx_to_offset(m_page, m_objidx, class->size);
+
+	vaddr = kmap_atomic(m_page);
+	target = (unsigned int *)((unsigned long)vaddr + m_offset);
+	if (!class->huge)
+		target = (unsigned int *)((unsigned long)target + ZS_HANDLE_SIZE);
+
+	switch (op) {
+	case OBJ_SEQ_GET:
+		ret = *target;
+		break;
+	case OBJ_SEQ_SET:
+		seq = pool->recent_seq++;
+		*target = seq;
+		seq_sum += seq;
+		objs_used++;
+		break;
+	case OBJ_SEQ_CLEAR:
+		seq_sum -= *target;
+		objs_used--;
+		break;
+	default:
+		break;
+	}
+
+	kunmap_atomic(vaddr);
+
+	return ret;
+}
+
+static inline int is_obj_writeback_suitable(unsigned int seq)
+{
+	return objs_used ? ((seq * 2) < (seq_sum / objs_used)) : 0;
+}
+#else
+static inline unsigned int obj_seq_operation(struct zs_pool *pool,
+		struct size_class *class, unsigned long obj, enum obj_seq_op op)
+{
+	return 0;
+}
+
+static inline int is_obj_writeback_suitable(unsigned int seq)
+{
+	return 1;
+}
+#endif
+
 /**
  * zs_map_object - get address of allocated object from handle.
  * @pool: pool from which the object was allocated
@@ -1276,7 +1511,15 @@ void *zs_map_object(struct zs_pool *pool
 	BUG_ON(in_interrupt());
 
 	/* From now on, migration cannot move the object */
+#ifdef CONFIG_ZSWAP_SAME_PAGE_SHARING
+	if (mm == ZS_MM_RO_NOWAIT) {
+		if (!trypin_tag(handle))
+			return NULL;
+	} else
+		pin_tag(handle);
+#else
 	pin_tag(handle);
+#endif
 
 	obj = handle_to_obj(handle);
 	obj_to_location(obj, &page, &obj_idx);
@@ -1302,6 +1545,9 @@ void *zs_map_object(struct zs_pool *pool
 out:
 	if (!class->huge)
 		ret += ZS_HANDLE_SIZE;
+#ifdef CONFIG_ZSMALLOC_OBJ_SEQ
+	ret += ZS_OBJ_SEQ_SIZE;
+#endif
 
 	return ret;
 }
@@ -1389,6 +1635,9 @@ unsigned long zs_malloc(struct zs_pool *
 	struct size_class *class;
 	struct page *first_page;
 
+#ifdef CONFIG_ZSMALLOC_OBJ_SEQ
+	size += ZS_OBJ_SEQ_SIZE;
+#endif
 	if (unlikely(!size || size > ZS_MAX_ALLOC_SIZE))
 		return 0;
 
@@ -1401,7 +1650,7 @@ unsigned long zs_malloc(struct zs_pool *
 	class = pool->size_class[get_size_class_index(size)];
 
 	spin_lock(&class->lock);
-	first_page = find_get_zspage(class);
+	first_page = find_available_zspage(class);
 
 	if (!first_page) {
 		spin_unlock(&class->lock);
@@ -1421,6 +1670,7 @@ unsigned long zs_malloc(struct zs_pool *
 	}
 
 	obj = obj_malloc(first_page, class, handle);
+	obj_seq_operation(pool, class, obj, OBJ_SEQ_SET);
 	/* Now move the zspage to another fullness group, if required */
 	fix_fullness_group(class, first_page);
 	record_obj(handle, obj);
@@ -1462,6 +1712,175 @@ static void obj_free(struct zs_pool *poo
 	zs_stat_dec(class, OBJ_USED, 1);
 }
 
+/*
+ * This tries to reclaim all the provided zspage's objects by calling the
+ * zs_pool's ops->evict function for each object in use. This requires
+ * the zspage's class lock to be held when calling this function. Since
+ * the evict function may sleep, this drops the class lock before evicting
+ * and objects. No other locks should be held when calling this function.
+ * This will return with the class lock unlocked.
+ *
+ * If there is no zs_pool->ops or ops->evict function, this returns error.
+ *
+ * This returns 0 on success, -err on failure. On failure, some of the
+ * objects may have been freed, but not all. On success, the entires zspage
+ * has been freed and should not be used anymore.
+ */
+
+#define ZS_RECLAIM_MAGIC 0xf1f2f3f45f6f7f8fULL
+static void obj_mark_to_free(struct zs_pool *pool, struct size_class *class,
+			unsigned long obj)
+{
+	struct page *first_page, *f_page;
+	unsigned long f_objidx, f_offset;
+	void *vaddr;
+	unsigned long *data;
+	int class_idx;
+	enum fullness_group fullness;
+
+	BUG_ON(!obj);
+
+	obj &= ~OBJ_ALLOCATED_TAG;
+	obj_to_location(obj, &f_page, &f_objidx);
+	first_page = get_first_page(f_page);
+
+	get_zspage_mapping(first_page, &class_idx, &fullness);
+	f_offset = obj_idx_to_offset(f_page, f_objidx, class->size);
+
+	vaddr = kmap_atomic(f_page);
+	data = (unsigned long *)(vaddr + f_offset);
+	if (!class->huge)
+		data = (unsigned long *)((unsigned long)data + ZS_HANDLE_SIZE);
+#ifdef CONFIG_ZSMALLOC_OBJ_SEQ
+	data = (unsigned long *)((unsigned long)data + ZS_OBJ_SEQ_SIZE);
+#endif
+	*data = (unsigned long)ZS_RECLAIM_MAGIC;
+	kunmap_atomic(vaddr);
+}
+
+static int is_obj_marked_to_free(struct zs_pool *pool, unsigned long handle)
+{
+	unsigned long *data;
+	unsigned long tmp;
+	int ret = 0;
+	data = zs_map_object(pool, handle, ZS_MM_RO);
+	if (*data == (unsigned long)ZS_RECLAIM_MAGIC)
+		ret = 1;
+	tmp = *data;
+	zs_unmap_object(pool, handle);
+	return ret;
+}
+
+static int reclaim_zspage(struct zs_pool *pool, struct page *first_page)
+{
+	struct size_class *class;
+	enum fullness_group fullness;
+	struct page *page = first_page;
+	unsigned long obj, handle;
+	void* vaddr;
+	int class_idx, ret = 0;
+	int freeable = 1;
+	int obj_reclaimed = 0;
+
+	BUG_ON(!is_first_page(first_page));
+
+	get_zspage_mapping(first_page, &class_idx, &fullness);
+	class = pool->size_class[class_idx];
+
+	assert_spin_locked(&class->lock);
+
+	if (!pool->ops || !pool->ops->evict) {
+		spin_unlock(&class->lock);
+		return -EINVAL;
+	}
+
+	/*
+	 * move the zspage into the reclaim fullness group,
+	 * so it's not available for use by zs_malloc,
+	 * and won't be freed by zs_free
+	 */
+	remove_zspage(first_page, class, fullness);
+	set_zspage_mapping(first_page, class_idx, ZS_RECLAIM);
+
+	spin_unlock(&class->lock);
+
+	might_sleep();
+
+	while (page) {
+		unsigned long offset, idx = 0;
+
+		while ((offset = obj_idx_to_offset(page, idx, class->size))
+					< PAGE_SIZE) {
+			int seq;
+#ifdef CONFIG_ZSMALLOC_OBJ_SEQ
+			obj_scanned++;
+#endif
+			obj = (unsigned long)location_to_obj(page, idx++);
+
+
+			vaddr = kmap_atomic(page);
+			if (class->huge)
+				handle = (unsigned long)page_private(page);
+			else
+				handle = obj_to_head(class, page, vaddr + offset);
+			kunmap_atomic(vaddr);
+
+			if (handle & OBJ_ALLOCATED_TAG)
+				handle &= ~OBJ_ALLOCATED_TAG;
+			else
+				continue;
+
+			seq = obj_seq_operation(pool, class, obj, OBJ_SEQ_GET);
+			if (!is_obj_writeback_suitable(seq)) {
+				freeable = 0;
+				continue;
+			}
+
+			ret = pool->ops->evict(pool, handle);
+			if (ret && !is_obj_marked_to_free(pool, handle)) {
+				spin_lock(&class->lock);
+				fix_fullness_group(class, first_page);
+				spin_unlock(&class->lock);
+				return ret;
+			}
+			obj_seq_operation(pool, class, obj, OBJ_SEQ_CLEAR);
+			obj_free(pool, class, obj);
+			free_handle(pool, handle);
+#ifdef CONFIG_ZSMALLOC_OBJ_SEQ
+			obj_success++;
+#endif
+			obj_reclaimed++;
+		}
+
+		page = get_next_page(page);
+	}
+
+	if (freeable)
+		free_zspage(first_page);
+	else {
+		spin_lock(&class->lock);
+		fix_fullness_group(class, first_page);
+		spin_unlock(&class->lock);
+		return obj_reclaimed ? 0 : -ENOENT;
+	}
+
+	zs_stat_dec(class, OBJ_ALLOCATED, get_maxobj_per_zspage(
+				class->size, class->pages_per_zspage));
+	atomic_long_sub(class->pages_per_zspage, &pool->pages_allocated);
+
+	return obj_reclaimed ? 0 : -ENOENT;
+}
+
+/**
+ * zs_free - Free the handle from this pool.
+ * @pool: pool containing the handle
+ * @obj: the handle to free
+ *
+ * The caller must provide a valid handle that is contained
+ * in the provided pool. The caller must ensure this is
+ * not called after evict() has returned successfully for the
+ * handle.
+ */
 void zs_free(struct zs_pool *pool, unsigned long handle)
 {
 	struct page *first_page, *f_page;
@@ -1482,6 +1901,17 @@ void zs_free(struct zs_pool *pool, unsig
 	class = pool->size_class[class_idx];
 
 	spin_lock(&class->lock);
+
+	/* must re-check fullness after taking class lock */
+	get_zspage_mapping(first_page, &class_idx, &fullness);
+	if (fullness == ZS_RECLAIM) {
+		spin_unlock(&class->lock);
+		obj_mark_to_free(pool, class, obj);
+		unpin_tag(handle);
+		return; /* will be freed during reclaim */
+	}
+
+	obj_seq_operation(pool, class, obj, OBJ_SEQ_CLEAR);
 	obj_free(pool, class, obj);
 	fullness = fix_fullness_group(class, first_page);
 	if (fullness == ZS_EMPTY) {
@@ -1498,6 +1928,41 @@ void zs_free(struct zs_pool *pool, unsig
 }
 EXPORT_SYMBOL_GPL(zs_free);
 
+/**
+ * zs_shrink - Shrink the pool
+ * @pool: pool to shrink
+ *
+ * The pool will be shrunk by one zspage, which is some
+ * number of pages in size. On success, the number of freed
+ * pages is returned. On failure, the error is returned.
+ */
+int zs_shrink(struct zs_pool *pool)
+{
+	struct size_class *class;
+	enum fullness_group fullness;
+	struct page *page;
+	int class_idx, ret;
+
+	if (!pool->ops || !pool->ops->evict)
+		return -EINVAL;
+
+	/* if a page is found, the class is locked */
+	page = find_lru_zspage(pool);
+	if (!page)
+		return -ENOENT;
+
+	get_zspage_mapping(page, &class_idx, &fullness);
+	class = pool->size_class[class_idx];
+
+	/* reclaim_zspage unlocks the class lock */
+	ret = reclaim_zspage(pool, page);
+	if (ret)
+		return ret;
+
+	return class->pages_per_zspage;
+}
+EXPORT_SYMBOL_GPL(zs_shrink);
+
 static void zs_object_copy(unsigned long src, unsigned long dst,
 				struct size_class *class)
 {
@@ -1533,12 +1998,7 @@ static void zs_object_copy(unsigned long
 		if (written == class->size)
 			break;
 
-		s_off += size;
-		s_size -= size;
-		d_off += size;
-		d_size -= size;
-
-		if (s_off >= PAGE_SIZE) {
+		if (s_off + size >= PAGE_SIZE) {
 			kunmap_atomic(d_addr);
 			kunmap_atomic(s_addr);
 			s_page = get_next_page(s_page);
@@ -1547,15 +2007,21 @@ static void zs_object_copy(unsigned long
 			d_addr = kmap_atomic(d_page);
 			s_size = class->size - written;
 			s_off = 0;
+		} else {
+			s_off += size;
+			s_size -= size;
 		}
 
-		if (d_off >= PAGE_SIZE) {
+		if (d_off + size >= PAGE_SIZE) {
 			kunmap_atomic(d_addr);
 			d_page = get_next_page(d_page);
 			BUG_ON(!d_page);
 			d_addr = kmap_atomic(d_page);
 			d_size = class->size - written;
 			d_off = 0;
+		} else {
+			d_off += size;
+			d_size -= size;
 		}
 	}
 
@@ -1641,6 +2107,13 @@ static int migrate_zspage(struct zs_pool
 		free_obj = obj_malloc(d_page, class, handle);
 		zs_object_copy(used_obj, free_obj, class);
 		index++;
+		/*
+		 * record_obj updates handle's value to free_obj and it will
+		 * invalidate lock bit(ie, HANDLE_PIN_BIT) of handle, which
+		 * breaks synchronization using pin_tag(e,g, zs_free) so
+		 * let's keep the lock bit.
+		 */
+		free_obj |= BIT(HANDLE_PIN_BIT);
 		record_obj(handle, free_obj);
 		unpin_tag(handle);
 		obj_free(pool, class, used_obj);
@@ -1660,7 +2133,7 @@ static struct page *alloc_target_page(st
 	int i;
 	struct page *page;
 
-	for (i = 0; i < _ZS_NR_FULLNESS_GROUPS; i++) {
+	for (i = 0; i < _ZS_NR_AVAILABLE_FULLNESS_GROUPS; i++) {
 		page = class->fullness_list[i];
 		if (page) {
 			remove_zspage(page, class, i);
@@ -1775,6 +2248,45 @@ unsigned long zs_compact(struct zs_pool
 }
 EXPORT_SYMBOL_GPL(zs_compact);
 
+/*
+ * zs_compactable - determine whether the given number of pages can be
+ * reclaimed from the pool by executing zs_compact
+ * @pool: the pool to compact
+ * @pages: number of pages to be reclaimed
+ */
+bool zs_compactable(struct zs_pool *pool, unsigned int pages)
+{
+#ifdef CONFIG_ZSMALLOC_STAT
+	int i, objs_per_zspage;
+	struct size_class *class;
+	unsigned int nr_reclaimable_zspages, total_reclaimable_pages = 0;
+	unsigned long obj_allocated, obj_used;
+
+	for (i = 0; i < zs_size_classes; i++) {
+		class = pool->size_class[i];
+		if (class->index != i)
+			continue;
+
+		spin_lock(&class->lock);
+		obj_allocated = zs_stat_get(class, OBJ_ALLOCATED);
+		obj_used = zs_stat_get(class, OBJ_USED);
+		spin_unlock(&class->lock);
+
+		objs_per_zspage = get_maxobj_per_zspage(class->size,
+				class->pages_per_zspage);
+		nr_reclaimable_zspages = (obj_allocated - obj_used) /
+				objs_per_zspage;
+		total_reclaimable_pages += nr_reclaimable_zspages *
+				class->pages_per_zspage;
+
+		if (total_reclaimable_pages >= pages)
+			return true;
+	}
+#endif
+	return false;
+}
+EXPORT_SYMBOL_GPL(zs_compactable);
+
 /**
  * zs_create_pool - Creates an allocation pool to work from.
  * @flags: allocation flags used to allocate pool metadata
@@ -1785,7 +2297,7 @@ EXPORT_SYMBOL_GPL(zs_compact);
  * On success, a pointer to the newly created pool is returned,
  * otherwise NULL.
  */
-struct zs_pool *zs_create_pool(char *name, gfp_t flags)
+struct zs_pool *zs_create_pool(char *name, gfp_t flags, struct zs_ops *ops)
 {
 	int i;
 	struct zs_pool *pool;
@@ -1856,10 +2368,15 @@ struct zs_pool *zs_create_pool(char *nam
 	}
 
 	pool->flags = flags;
+	pool->ops = ops;
 
 	if (zs_pool_stat_create(name, pool))
 		goto err;
 
+#ifdef CONFIG_ZSMALLOC_OBJ_SEQ
+	pool->recent_seq = 1;
+#endif
+
 	return pool;
 
 err:
