--- original/include/linux/zsmalloc.h	2020-05-07 08:06:21.606189919 +0200
+++ changed/include/linux/zsmalloc.h	2019-06-03 13:39:53.000000000 +0200
@@ -25,7 +25,10 @@
 enum zs_mapmode {
 	ZS_MM_RW, /* normal read-write mapping */
 	ZS_MM_RO, /* read-only (no copy-out at unmap time) */
-	ZS_MM_WO /* write-only (no copy-in at map time) */
+	ZS_MM_WO, /* write-only (no copy-in at map time) */
+#ifdef CONFIG_ZSWAP_SAME_PAGE_SHARING
+	ZS_MM_RO_NOWAIT /* read-only (no wait if the handle is busy)*/
+#endif
 	/*
 	 * NOTE: ZS_MM_WO should only be used for initializing new
 	 * (uninitialized) allocations.  Partial writes to already
@@ -36,11 +39,16 @@ enum zs_mapmode {
 
 struct zs_pool;
 
-struct zs_pool *zs_create_pool(char *name, gfp_t flags);
+struct zs_ops {
+	int (*evict)(struct zs_pool *pool, unsigned long handle);
+};
+
+struct zs_pool *zs_create_pool(char *name, gfp_t flags, struct zs_ops *ops);
 void zs_destroy_pool(struct zs_pool *pool);
 
 unsigned long zs_malloc(struct zs_pool *pool, size_t size);
 void zs_free(struct zs_pool *pool, unsigned long obj);
+int zs_shrink(struct zs_pool *pool);
 
 void *zs_map_object(struct zs_pool *pool, unsigned long handle,
 			enum zs_mapmode mm);
@@ -48,5 +56,6 @@ void zs_unmap_object(struct zs_pool *poo
 
 unsigned long zs_get_total_pages(struct zs_pool *pool);
 unsigned long zs_compact(struct zs_pool *pool);
+bool zs_compactable(struct zs_pool *pool, unsigned int pages);
 
 #endif
